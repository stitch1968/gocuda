//go:build ignore
// +build ignore

package main

import (
	"context"
	"fmt"
	"log"

	cuda "github.com/stitch1968/gocuda"
)

func main() {
	// Initialize CUDA
	if err := cuda.Initialize(); err != nil {
		log.Fatal("Failed to initialize CUDA:", err)
	}

	fmt.Println("ðŸš€ GoCUDA - Go routine-like CUDA Interface")
	fmt.Println("===========================================")

	// Show device information
	devices, _ := cuda.GetDevices()
	for _, device := range devices {
		fmt.Printf("ðŸ“± Device %d: %s\n", device.ID, device.Name)
		fmt.Printf("   Memory: %d MB\n", device.Properties.TotalGlobalMem/(1024*1024))
		fmt.Printf("   Cores: %d\n", device.Properties.MultiProcessorCount)
	}

	fmt.Println("\nðŸ”¥ Demo: Basic cuda.Go() usage (like Go routines)")
	// Execute function on GPU like a Go routine
	cuda.Go(func(ctx context.Context, args ...interface{}) error {
		message := args[0].(string)
		number := args[1].(int)
		fmt.Printf("   âœ… Hello from GPU! Message: %s, Number: %d\n", message, number)
		return nil
	}, "Hello World", 42)
	cuda.Synchronize()

	fmt.Println("\nðŸ§® Demo: Vector Addition on GPU")
	n := 100000

	// Create input vectors
	a := make([]float32, n)
	b := make([]float32, n)

	// Fill vectors with data
	for i := 0; i < n; i++ {
		a[i] = 1.5
		b[i] = 2.5
	}

	// Perform vector addition using Simple API
	result, err := cuda.SimpleVectorAdd(a, b)
	if err != nil {
		log.Fatal(err)
	}

	fmt.Printf("   âœ… Added %d vectors: 1.5 + 2.5 = %.1f (result length: %d)\n", n, result[0], len(result))

	fmt.Println("\nâš¡ Demo: Parallel Processing")
	sum := 0
	cuda.ParallelFor(0, 1000, func(i int) error {
		sum += i // Simulate parallel work
		return nil
	})
	fmt.Printf("   âœ… Processed 1000 elements in parallel\n")

	fmt.Println("\nðŸ“¡ Demo: CUDA Channels")
	ch := cuda.NewCudaChannel(5)
	defer ch.Close()

	// Send from GPU
	cuda.Go(func(ctx context.Context, args ...interface{}) error {
		channel := args[0].(*cuda.CudaChannel)
		channel.Send("Message from GPU kernel!")
		return nil
	}, ch)
	cuda.Synchronize()

	// Receive on host
	message := ch.Receive()
	fmt.Printf("   âœ… Received: %v\n", message)

	fmt.Println("\nðŸŽ¯ Demo: Multiple Streams (Concurrent Execution)")
	ctx := cuda.GetDefaultContext()
	stream1, _ := ctx.NewStream()
	stream2, _ := ctx.NewStream()

	// Execute different tasks concurrently
	cuda.GoWithStream(stream1, func(ctx context.Context, args ...interface{}) error {
		fmt.Printf("   âœ… Task 1 running on Stream 1\n")
		return nil
	})

	cuda.GoWithStream(stream2, func(ctx context.Context, args ...interface{}) error {
		fmt.Printf("   âœ… Task 2 running on Stream 2\n")
		return nil
	})

	stream1.Synchronize()
	stream2.Synchronize()

	fmt.Println("\nðŸ’¾ Demo: Memory Management")
	free, total := cuda.GetMemoryInfo()
	fmt.Printf("   ðŸ“Š GPU Memory - Free: %d MB, Total: %d MB\n",
		free/(1024*1024), total/(1024*1024))

	fmt.Println("\nðŸŽ‰ All demos completed successfully!")
	fmt.Println("\nKey Features:")
	fmt.Println("â€¢ cuda.Go() - Execute functions on GPU like Go routines")
	fmt.Println("â€¢ cuda.CudaChannel - Channel-like communication")
	fmt.Println("â€¢ cuda.ParallelFor() - Parallel processing patterns")
	fmt.Println("â€¢ Multiple streams for concurrent execution")
	fmt.Println("â€¢ Automatic memory management with finalizers")
	fmt.Println("â€¢ Built-in kernels: VectorAdd, MatrixMultiply, Convolution")
	fmt.Println("â€¢ cuda.CudaWaitGroup - Like sync.WaitGroup for GPU")
	fmt.Println("\nAPI Comparison:")
	fmt.Println("Go:      go func() { ... }()")
	fmt.Println("GoCUDA:  cuda.Go(func(ctx context.Context, args ...interface{}) error { ... }, args...)")
}
