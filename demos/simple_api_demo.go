//go:build ignore
// +build ignore

package main

import (
	"fmt"
	"time"

	cuda "github.com/stitch1968/gocuda"
)

func main() {
	fmt.Println("=== GoCUDA Simple API Demo ===")
	fmt.Println("This demo showcases user-friendly patterns and improvements")
	fmt.Println("Works on any system - GPU optional!")

	// Demo 1: Enhanced Error Handling
	fmt.Println("ï¿½ï¸ Demo 1: Enhanced Error Handling")
	demoErrorHandling()
	fmt.Println()

	// Demo 2: Simple Memory Management
	fmt.Println("ğŸ’¾ Demo 2: Memory Management with Builder Pattern")
	demoMemoryManagement()
	fmt.Println()

	// Demo 3: Working API Examples
	fmt.Println("âš¡ Demo 3: Current Working API")
	demoWorkingAPI()
	fmt.Println()

	// Demo 4: Performance Patterns
	fmt.Println("ğŸ Demo 4: Performance Patterns")
	demoPerformancePatterns()
	fmt.Println()

	fmt.Println("âœ… All demos completed successfully!")
	fmt.Println("ğŸ’¡ This shows improved error handling and patterns")
	fmt.Println("ğŸ“š Next: Check docs/QUICK_START.md for detailed learning path")
}

func demoErrorHandling() {
	fmt.Println("   Demonstrating enhanced error handling...")

	// Create enhanced error examples
	err := cuda.NewMemoryError("GPU memory allocation",
		"Insufficient GPU memory",
		8*1024*1024*1024, // requested: 8GB
		2*1024*1024*1024) // available: 2GB

	fmt.Printf("   ğŸ“‹ Error: %s\n", err.Error())
	fmt.Printf("   ï¿½ Details: %v\n", err.GetDetails())

	if err.IsRecoverable() {
		fmt.Println("   ğŸ”„ Error is recoverable")
	}

	// Create kernel error example
	gridDim := cuda.Dim3{X: 1536, Y: 1, Z: 1}
	blockDim := cuda.Dim3{X: 1024, Y: 1024, Z: 64}
	kernelErr := cuda.NewKernelError("Kernel launch",
		"Block size too large for device",
		gridDim, blockDim)

	fmt.Printf("   ğŸ“‹ Kernel Error: %s\n", kernelErr.Error())
	fmt.Printf("   ğŸ’¡ Details: %v\n", kernelErr.GetDetails())

	fmt.Println("   âœ… Enhanced error handling provides actionable guidance")
}

func demoMemoryManagement() {
	fmt.Println("   Demonstrating builder pattern memory allocation...")

	// Pattern 1: Device memory allocation
	fmt.Println("   ğŸ“ Pattern 1: Device memory")
	mem1, err := cuda.Alloc(1024).OnDevice().Allocate()
	if err != nil {
		fmt.Printf("   âš ï¸  Device allocation failed: %v\n", err)
	} else {
		fmt.Println("   âœ… Device memory allocated successfully")
		defer mem1.Free()
	}

	// Pattern 2: Pinned memory allocation
	fmt.Println("   ğŸ“ Pattern 2: Pinned memory")
	mem2, err := cuda.Alloc(512).Pinned().Allocate()
	if err != nil {
		fmt.Printf("   âš ï¸  Pinned allocation failed: %v\n", err)
	} else {
		fmt.Println("   âœ… Pinned memory allocated successfully")
		defer mem2.Free()
	}

	// Pattern 3: Unified memory allocation
	fmt.Println("   ğŸ“ Pattern 3: Unified memory")
	mem3, err := cuda.Alloc(256).Unified().Allocate()
	if err != nil {
		fmt.Printf("   âš ï¸  Unified allocation failed: %v\n", err)
	} else {
		fmt.Println("   âœ… Unified memory allocated successfully")
		defer mem3.Free()
	}

	fmt.Println("   ğŸ’¡ Builder pattern provides clear, chainable memory allocation")
}

func demoWorkingAPI() {
	fmt.Println("   Using the current working API with improvements...")

	// Example 1: Map operation (works)
	fmt.Println("   ğŸ“ Map operation:")
	input := []float64{1, 2, 3, 4, 5}
	result, err := cuda.Map(input, func(x float64) float64 {
		return x * x // Square each element
	})
	if err != nil {
		fmt.Printf("   âŒ Map operation failed: %v\n", err)
	} else {
		fmt.Printf("   Input:  %v\n", input)
		fmt.Printf("   Output: %v (squared)\n", result)
		fmt.Println("   âœ… Map operation completed successfully")
	}

	// Example 2: Reduce operation (works)
	fmt.Println("   ğŸ“ Reduce operation:")
	numbers := []float64{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}
	sum, err := cuda.Reduce(numbers, func(a, b float64) float64 { return a + b })
	if err != nil {
		fmt.Printf("   âŒ Reduce operation failed: %v\n", err)
	} else {
		fmt.Printf("   Sum of 1-10: %.0f\n", sum)
		fmt.Println("   âœ… Reduce operation completed successfully")
	}

	// Example 3: Parallel processing (works)
	fmt.Println("   ğŸ“ Parallel processing:")
	processed := 0
	err = cuda.ParallelFor(0, 100, func(i int) error {
		processed++
		return nil
	})
	if err != nil {
		fmt.Printf("   âŒ Parallel processing failed: %v\n", err)
	} else {
		fmt.Printf("   âœ… Processed %d elements in parallel\n", processed)
	}
}

func demoPerformancePatterns() {
	fmt.Println("   Demonstrating performance measurement patterns...")

	// Create test data
	size := 1000
	data := make([]float64, size)
	for i := 0; i < size; i++ {
		data[i] = float64(i)
	}

	// CPU timing
	fmt.Printf("   ğŸ–¥ï¸  CPU computation (%d elements)...\n", size)
	startCPU := time.Now()
	cpuResult := make([]float64, size)
	for i := 0; i < size; i++ {
		cpuResult[i] = data[i] * data[i]
	}
	cpuTime := time.Since(startCPU)

	// GPU timing (using existing Map function)
	fmt.Printf("   ğŸš€ GPU computation (%d elements)...\n", size)
	startGPU := time.Now()
	gpuResult, err := cuda.Map(data, func(x float64) float64 {
		return x * x
	})
	gpuTime := time.Since(startGPU)

	if err != nil {
		fmt.Printf("   âš ï¸  GPU computation failed: %v\n", err)
		fmt.Printf("   â±ï¸  CPU time: %v\n", cpuTime)
		return
	}

	// Verify results match
	matches := len(cpuResult) == len(gpuResult)
	if matches {
		for i := 0; i < len(cpuResult) && i < len(gpuResult); i++ {
			if cpuResult[i] != gpuResult[i] {
				matches = false
				break
			}
		}
	}

	// Display results
	fmt.Printf("   â±ï¸  CPU time: %v\n", cpuTime)
	fmt.Printf("   â±ï¸  GPU time: %v\n", gpuTime)

	if matches {
		fmt.Println("   âœ… Results match between CPU and GPU")
	} else {
		fmt.Println("   âš ï¸  Results differ")
	}

	if gpuTime < cpuTime {
		speedup := float64(cpuTime) / float64(gpuTime)
		fmt.Printf("   ğŸš€ GPU is %.2fx faster than CPU\n", speedup)
	} else {
		slowdown := float64(gpuTime) / float64(cpuTime)
		fmt.Printf("   ğŸŒ GPU is %.2fx slower (overhead for small data)\n", slowdown)
		fmt.Println("   ğŸ’¡ GPU benefits increase with larger datasets")
	}
}
