# üöÄ Expert Module 1: Custom Kernel Development & Assembly Optimization

**Goal:** Master custom GPU kernel development and achieve theoretical performance limits through assembly-level optimization

---

## üìö Learning Objectives

By the end of this module, you will:
- üî• **Design custom kernels** optimized for specific GPU architectures
- ‚ö° **Achieve 95%+ theoretical performance** through assembly optimization
- üß† **Master GPU microarchitecture** - warps, SMs, memory hierarchy
- üîß **Implement advanced algorithms** not available in standard libraries
- üìä **Profile and debug** at the instruction level

---

## üèóÔ∏è Theoretical Foundation

### GPU Microarchitecture Deep Dive

**Streaming Multiprocessor (SM) Architecture:**
```
SM (Streaming Multiprocessor)
‚îú‚îÄ‚îÄ Warp Schedulers (4x)
‚îú‚îÄ‚îÄ CUDA Cores (128x FP32)
‚îú‚îÄ‚îÄ Tensor Cores (4x)
‚îú‚îÄ‚îÄ Special Function Units (16x)
‚îú‚îÄ‚îÄ Load/Store Units (32x)
‚îú‚îÄ‚îÄ Shared Memory (96KB)
‚îî‚îÄ‚îÄ L1 Cache (128KB)
```

**Memory Hierarchy Latencies:**
- **Registers**: 0 cycles (when available)
- **Shared Memory**: ~20 cycles
- **L1 Cache**: ~25 cycles  
- **L2 Cache**: ~200 cycles
- **Global Memory**: ~400 cycles
- **System Memory**: ~800 cycles

### Kernel Launch Mechanics

```go
// Kernel execution model
GridDim √ó BlockDim = Total Threads
Warps = ceil(BlockDim / 32)
Active Warps per SM = min(Max Warps, Available Resources)
```

---

## üî• Chapter 1: High-Performance Kernel Templates

### Matrix Multiplication Kernel (Optimized)

Create `kernels/gemm_optimized.go`:

```go
package main

import (
    "fmt"
    "time"
    "unsafe"
    "github.com/stitch1968/gocuda"
    "github.com/stitch1968/gocuda/memory"
    "github.com/stitch1968/gocuda/hardware"
)

// High-performance GEMM kernel implementation
type OptimizedGEMM struct {
    ctx        *cuda.Context
    stream     *cuda.Stream
    blockSizeX int
    blockSizeY int
    tileSize   int
}

func NewOptimizedGEMM() *OptimizedGEMM {
    ctx := cuda.GetDefaultContext()
    stream, _ := ctx.NewStream()
    
    return &OptimizedGEMM{
        ctx:        ctx,
        stream:     stream,
        blockSizeX: 16, // Optimized for current architectures
        blockSizeY: 16,
        tileSize:   16,
    }
}

func (g *OptimizedGEMM) MatMul(A, B, C *memory.DeviceMemory, M, N, K int) error {
    // Launch configuration
    gridX := (N + g.blockSizeX - 1) / g.blockSizeX
    gridY := (M + g.blockSizeY - 1) / g.blockSizeY
    
    // Shared memory size calculation
    sharedMemSize := 2 * g.tileSize * g.tileSize * 4 // 2 tiles √ó elements √ó sizeof(float32)
    
    fmt.Printf("üöÄ Launching GEMM kernel: Grid(%d,%d), Block(%d,%d), SharedMem: %d bytes\n",
               gridX, gridY, g.blockSizeX, g.blockSizeY, sharedMemSize)
    
    // Kernel parameters
    params := []interface{}{
        A.Ptr(), B.Ptr(), C.Ptr(),
        int32(M), int32(N), int32(K),
    }
    
    // Launch optimized GEMM kernel
    return g.launchGEMMKernel(gridX, gridY, params, sharedMemSize)
}

func (g *OptimizedGEMM) launchGEMMKernel(gridX, gridY int, params []interface{}, sharedMem int) error {
    // This would launch actual CUDA kernel in real implementation
    // For GoCUDA simulation, we implement equivalent optimized logic
    
    A := params[0].(*memory.DeviceMemory)
    B := params[1].(*memory.DeviceMemory)  
    C := params[2].(*memory.DeviceMemory)
    M := params[3].(int32)
    N := params[4].(int32)
    K := params[5].(int32)
    
    // Simulate high-performance GEMM computation
    return g.simulateOptimizedGEMM(A, B, C, int(M), int(N), int(K))
}

func (g *OptimizedGEMM) simulateOptimizedGEMM(A, B, C *memory.DeviceMemory, M, N, K int) error {
    // Simulate tiled matrix multiplication with optimizations:
    // 1. Shared memory tiling
    // 2. Coalesced memory access  
    // 3. Register blocking
    // 4. Warp-level optimizations
    
    // Get matrix data (in real implementation, this would be GPU kernels)
    matrixA := make([]float32, M*K)
    matrixB := make([]float32, K*N)
    matrixC := make([]float32, M*N)
    
    A.CopyToHost(matrixA)
    B.CopyToHost(matrixB)
    
    // Optimized computation with tiling
    tileSize := g.tileSize
    
    for by := 0; by < M; by += tileSize {
        for bx := 0; bx < N; bx += tileSize {
            // Process tile
            for k := 0; k < K; k += tileSize {
                // Inner tile computation (simulates shared memory usage)
                g.computeTile(matrixA, matrixB, matrixC, M, N, K, by, bx, k, tileSize)
            }
        }
    }
    
    return C.CopyFromHost(matrixC)
}

func (g *OptimizedGEMM) computeTile(A, B, C []float32, M, N, K, tileY, tileX, tileK, tileSize int) {
    // Simulate optimized tile computation with:
    // - Shared memory usage simulation
    // - Coalesced access patterns
    // - Register reuse
    
    endY := min(tileY+tileSize, M)
    endX := min(tileX+tileSize, N)
    endK := min(tileK+tileSize, K)
    
    for i := tileY; i < endY; i++ {
        for j := tileX; j < endX; j++ {
            var sum float32
            for k := tileK; k < endK; k++ {
                sum += A[i*K+k] * B[k*N+j]
            }
            C[i*N+j] += sum
        }
    }
}

// Performance benchmarking
func (g *OptimizedGEMM) Benchmark(sizes []int) {
    fmt.Println("üìä GEMM Performance Benchmark:")
    fmt.Println("Size\tTime\t\tGFLOPs\tEfficiency")
    
    for _, size := range sizes {
        M, N, K := size, size, size
        
        // Allocate matrices
        A, _ := memory.Alloc(int64(M * K * 4))
        B, _ := memory.Alloc(int64(K * N * 4))
        C, _ := memory.Alloc(int64(M * N * 4))
        defer A.Free()
        defer B.Free()
        defer C.Free()
        
        // Initialize with test data
        g.initializeTestData(A, B, M, N, K)
        
        // Benchmark
        start := time.Now()
        g.MatMul(A, B, C, M, N, K)
        elapsed := time.Since(start)
        
        // Calculate performance metrics
        operations := float64(2) * float64(M) * float64(N) * float64(K)
        gflops := (operations / elapsed.Seconds()) / 1e9
        
        // Theoretical peak for modern GPUs: ~20-40 TFLOPS
        theoreticalPeak := 20000.0 // GFLOPS
        efficiency := (gflops / theoreticalPeak) * 100
        
        fmt.Printf("%d\t%v\t%.1f\t%.1f%%\n", size, elapsed, gflops, efficiency)
    }
}

func (g *OptimizedGEMM) initializeTestData(A, B *memory.DeviceMemory, M, N, K int) {
    // Initialize with random test data
    dataA := make([]float32, M*K)
    dataB := make([]float32, K*N)
    
    for i := range dataA {
        dataA[i] = float32(i%100) / 100.0
    }
    for i := range dataB {
        dataB[i] = float32(i%100) / 100.0
    }
    
    A.CopyFromHost(dataA)
    B.CopyFromHost(dataB)
}

func (g *OptimizedGEMM) Destroy() {
    g.stream.Destroy()
}

func min(a, b int) int {
    if a < b {
        return a
    }
    return b
}

// Demonstration
func main() {
    cuda.Initialize()
    fmt.Println("üî• Expert Kernel Development: Optimized GEMM")
    
    gemm := NewOptimizedGEMM()
    defer gemm.Destroy()
    
    // Performance benchmark
    sizes := []int{128, 256, 512, 1024}
    gemm.Benchmark(sizes)
    
    // Detailed analysis for one size
    analyzeKernelPerformance(gemm, 512)
}

func analyzeKernelPerformance(gemm *OptimizedGEMM, size int) {
    fmt.Printf("\nüîç Detailed Analysis for %dx%d GEMM:\n", size, size)
    
    M, N, K := size, size, size
    
    A, _ := memory.Alloc(int64(M * K * 4))
    B, _ := memory.Alloc(int64(K * N * 4))
    C, _ := memory.Alloc(int64(M * N * 4))
    defer A.Free()
    defer B.Free()
    defer C.Free()
    
    gemm.initializeTestData(A, B, M, N, K)
    
    // Warm-up
    gemm.MatMul(A, B, C, M, N, K)
    
    // Multiple runs for accuracy
    const runs = 10
    times := make([]time.Duration, runs)
    
    for i := 0; i < runs; i++ {
        start := time.Now()
        gemm.MatMul(A, B, C, M, N, K)
        times[i] = time.Since(start)
    }
    
    // Calculate statistics
    var totalTime time.Duration
    for _, t := range times {
        totalTime += t
    }
    avgTime := totalTime / runs
    
    operations := float64(2) * float64(M) * float64(N) * float64(K)
    avgGFLOPs := (operations / avgTime.Seconds()) / 1e9
    
    // Memory bandwidth analysis
    bytesTransferred := float64(M*K + K*N + M*N) * 4 // sizeof(float32)
    bandwidth := (bytesTransferred / avgTime.Seconds()) / (1024 * 1024 * 1024) // GB/s
    
    fmt.Printf("Average Time: %v\n", avgTime)
    fmt.Printf("Performance: %.1f GFLOPS\n", avgGFLOPs)
    fmt.Printf("Memory Bandwidth: %.1f GB/s\n", bandwidth)
    fmt.Printf("Arithmetic Intensity: %.2f FLOP/byte\n", operations/bytesTransferred)
    
    // GPU utilization estimation
    device := cuda.GetDevice(0)
    props := device.GetProperties()
    fmt.Printf("GPU: %s\n", props.Name)
    fmt.Printf("Compute Capability: %d.%d\n", props.ComputeCapability[0], props.ComputeCapability[1])
    
    // Optimization recommendations
    fmt.Println("\nüí° Optimization Analysis:")
    
    if bandwidth < 500 { // Typical GPU memory bandwidth > 500 GB/s
        fmt.Println("  - Memory-bound: Optimize memory access patterns")
        fmt.Println("  - Consider larger tiles to improve cache reuse")
    }
    
    if avgGFLOPs < 1000 { // Should achieve much higher for large matrices
        fmt.Println("  - Compute-bound: Increase arithmetic intensity")
        fmt.Println("  - Consider tensor core utilization")
    }
    
    fmt.Println("  - Monitor warp occupancy and register usage")
    fmt.Println("  - Profile for memory coalescing efficiency")
}
```

---

## ‚ö° Chapter 2: Warp-Level Programming

### Advanced Warp Primitives Implementation

Create `kernels/warp_advanced.go`:

```go
package main

import (
    "fmt"
    "github.com/stitch1968/gocuda"
    "github.com/stitch1968/gocuda/memory"
    "github.com/stitch1968/gocuda/hardware"
)

// Expert-level warp programming techniques
type WarpExpert struct {
    ctx    *cuda.Context
    stream *cuda.Stream
}

func NewWarpExpert() *WarpExpert {
    ctx := cuda.GetDefaultContext()
    stream, _ := ctx.NewStream()
    
    return &WarpExpert{
        ctx:    ctx,
        stream: stream,
    }
}

// Warp-optimized reduction with shuffle operations
func (w *WarpExpert) WarpReduceSum(data *memory.DeviceMemory, size int) (float32, error) {
    fmt.Println("üåä Expert Warp Reduction")
    
    // This demonstrates warp-level optimization techniques
    // In actual implementation, this would be a custom kernel
    
    // Simulate warp shuffle-based reduction
    hostData := make([]float32, size)
    data.CopyToHost(hostData)
    
    // Simulate warp-level parallel reduction
    result := w.simulateWarpReduce(hostData)
    
    fmt.Printf("Reduced %d elements to %.6f using warp shuffles\n", size, result)
    return result, nil
}

func (w *WarpExpert) simulateWarpReduce(data []float32) float32 {
    // Simulate highly optimized warp-level reduction
    // Real implementation would use __shfl_down_sync
    
    warpSize := 32
    numWarps := (len(data) + warpSize - 1) / warpSize
    warpSums := make([]float32, numWarps)
    
    // Phase 1: Warp-level reductions
    for warpId := 0; warpId < numWarps; warpId++ {
        start := warpId * warpSize
        end := min(start+warpSize, len(data))
        
        var warpSum float32
        for i := start; i < end; i++ {
            warpSum += data[i]
        }
        
        // Simulate warp shuffle reduction
        warpSum = w.simulateWarpShuffle(warpSum, end-start)
        warpSums[warpId] = warpSum
    }
    
    // Phase 2: Final reduction
    var totalSum float32
    for _, warpSum := range warpSums {
        totalSum += warpSum
    }
    
    return totalSum
}

func (w *WarpExpert) simulateWarpShuffle(value float32, activeThreads int) float32 {
    // Simulate warp shuffle-based reduction
    // This would be implemented as:
    // for (int offset = warpSize/2; offset > 0; offset /= 2) {
    //     value += __shfl_down_sync(0xFFFFFFFF, value, offset);
    // }
    
    return value // Simplified for simulation
}

// Warp-level matrix transpose
func (w *WarpExpert) WarpTranspose(input, output *memory.DeviceMemory, rows, cols int) error {
    fmt.Printf("üîÑ Warp-optimized transpose (%dx%d)\n", rows, cols)
    
    // Simulate warp-coalesced transpose
    inputData := make([]float32, rows*cols)
    input.CopyToHost(inputData)
    
    outputData := make([]float32, rows*cols)
    
    // Simulate tiled transpose with warp-level optimizations
    tileSize := 32 // Match warp size
    
    for tileRow := 0; tileRow < rows; tileRow += tileSize {
        for tileCol := 0; tileCol < cols; tileCol += tileSize {
            w.transposeTile(inputData, outputData, rows, cols, tileRow, tileCol, tileSize)
        }
    }
    
    return output.CopyFromHost(outputData)
}

func (w *WarpExpert) transposeTile(input, output []float32, rows, cols, tileRow, tileCol, tileSize int) {
    // Simulate shared memory tiled transpose
    endRow := min(tileRow+tileSize, rows)
    endCol := min(tileCol+tileSize, cols)
    
    // Simulate coalesced reads and writes
    for i := tileRow; i < endRow; i++ {
        for j := tileCol; j < endCol; j++ {
            if i < rows && j < cols {
                output[j*rows+i] = input[i*cols+j]
            }
        }
    }
}

// Advanced warp vote operations
func (w *WarpExpert) WarpCollectiveOperations(data *memory.DeviceMemory, size int) error {
    fmt.Println("üó≥Ô∏è Advanced Warp Collective Operations")
    
    hostData := make([]float32, size)
    data.CopyToHost(hostData)
    
    // Simulate various warp collective operations
    w.simulateWarpBallot(hostData)
    w.simulateWarpMatch(hostData)
    w.simulateWarpSync()
    
    return nil
}

func (w *WarpExpert) simulateWarpBallot(data []float32) {
    fmt.Println("  üéØ Warp Ballot Operation:")
    
    warpSize := 32
    for warpId := 0; warpId < (len(data)+warpSize-1)/warpSize; warpId++ {
        start := warpId * warpSize
        end := min(start+warpSize, len(data))
        
        // Simulate ballot - count elements > 0.5
        var ballot uint32
        positiveCount := 0
        
        for i := start; i < end; i++ {
            if data[i] > 0.5 {
                ballot |= (1 << uint(i-start))
                positiveCount++
            }
        }
        
        fmt.Printf("    Warp %d: %d/%d threads positive (ballot: 0x%08x)\n", 
                   warpId, positiveCount, end-start, ballot)
    }
}

func (w *WarpExpert) simulateWarpMatch(data []float32) {
    fmt.Println("  üéØ Warp Match Operation:")
    
    // Simulate __match_any_sync - find threads with same value
    valueGroups := make(map[float32][]int)
    
    for i, value := range data {
        if i < 32 { // First warp only for demo
            rounded := float32(int(value*10)) / 10 // Round for grouping
            valueGroups[rounded] = append(valueGroups[rounded], i)
        }
    }
    
    for value, threads := range valueGroups {
        if len(threads) > 1 {
            fmt.Printf("    Threads %v have matching value %.1f\n", threads, value)
        }
    }
}

func (w *WarpExpert) simulateWarpSync() {
    fmt.Println("  üéØ Warp Synchronization:")
    fmt.Println("    All warps synchronized using __syncwarp()")
    fmt.Println("    Memory ordering enforced across warp")
}

func (w *WarpExpert) Destroy() {
    w.stream.Destroy()
}

// Demonstration and benchmarking
func main() {
    cuda.Initialize()
    fmt.Println("‚ö° Expert Warp-Level Programming")
    
    expert := NewWarpExpert()
    defer expert.Destroy()
    
    // Test data
    size := 100000
    testData, _ := memory.Alloc(int64(size * 4))
    defer testData.Free()
    
    // Initialize test data
    hostData := make([]float32, size)
    for i := range hostData {
        hostData[i] = float32(i%1000) / 1000.0
    }
    testData.CopyFromHost(hostData)
    
    // Demonstrate warp operations
    fmt.Println("\n1. Warp Reduction:")
    result, _ := expert.WarpReduceSum(testData, size)
    fmt.Printf("   Final result: %.6f\n", result)
    
    fmt.Println("\n2. Warp Transpose:")
    rows, cols := 256, 256
    input, _ := memory.Alloc(int64(rows * cols * 4))
    output, _ := memory.Alloc(int64(rows * cols * 4))
    defer input.Free()
    defer output.Free()
    
    // Initialize matrix
    matrix := make([]float32, rows*cols)
    for i := range matrix {
        matrix[i] = float32(i)
    }
    input.CopyFromHost(matrix)
    
    expert.WarpTranspose(input, output, rows, cols)
    
    fmt.Println("\n3. Warp Collective Operations:")
    expert.WarpCollectiveOperations(testData, min(size, 128)) // Limit for demo
    
    // Performance analysis
    analyzeWarpPerformance(expert, testData, size)
}

func analyzeWarpPerformance(expert *WarpExpert, data *memory.DeviceMemory, size int) {
    fmt.Println("\nüìä Warp Performance Analysis:")
    
    device := cuda.GetDevice(0)
    props := device.GetProperties()
    
    fmt.Printf("Device: %s\n", props.Name)
    fmt.Printf("Warp Size: %d\n", props.WarpSize)
    fmt.Printf("Max Threads per Block: %d\n", props.MaxThreadsPerBlock)
    fmt.Printf("Multiprocessors: %d\n", props.MultiprocessorCount)
    
    // Calculate optimal configuration
    elementsPerWarp := props.WarpSize
    optimalBlocks := (size + props.MaxThreadsPerBlock - 1) / props.MaxThreadsPerBlock
    optimalWarps := optimalBlocks * (props.MaxThreadsPerBlock / props.WarpSize)
    
    fmt.Printf("\nOptimal Configuration for %d elements:\n", size)
    fmt.Printf("  Blocks: %d\n", optimalBlocks)
    fmt.Printf("  Warps per block: %d\n", props.MaxThreadsPerBlock/props.WarpSize)
    fmt.Printf("  Total warps: %d\n", optimalWarps)
    fmt.Printf("  Elements per warp: %d\n", elementsPerWarp)
    
    // Warp efficiency analysis
    activeWarps := (size + elementsPerWarp - 1) / elementsPerWarp
    warpEfficiency := float64(size) / float64(activeWarps*elementsPerWarp) * 100
    
    fmt.Printf("\nWarp Efficiency Analysis:\n")
    fmt.Printf("  Active warps: %d\n", activeWarps)
    fmt.Printf("  Warp efficiency: %.1f%%\n", warpEfficiency)
    
    if warpEfficiency < 90 {
        fmt.Println("  ‚ö†Ô∏è Low warp efficiency - consider padding data")
    } else {
        fmt.Println("  ‚úÖ Good warp efficiency")
    }
    
    fmt.Println("\nüí° Warp Optimization Tips:")
    fmt.Println("  - Ensure data size is multiple of warp size")
    fmt.Println("  - Use warp shuffle for efficient reductions")
    fmt.Println("  - Minimize warp divergence in conditionals")
    fmt.Println("  - Coalesce memory accesses within warps")
}

func min(a, b int) int {
    if a < b {
        return a
    }
    return b
}
```

---

## üß† Chapter 3: Memory Optimization at Kernel Level

Create `kernels/memory_expert.go`:

```go
package main

import (
    "fmt"
    "time"
    "github.com/stitch1968/gocuda"
    "github.com/stitch1968/gocuda/memory"
)

// Expert memory optimization techniques
type MemoryExpert struct {
    ctx    *cuda.Context
    stream *cuda.Stream
}

func NewMemoryExpert() *MemoryExpert {
    ctx := cuda.GetDefaultContext()
    stream, _ := ctx.NewStream()
    
    return &MemoryExpert{
        ctx:    ctx,
        stream: stream,
    }
}

// Demonstrate advanced coalescing optimization
func (m *MemoryExpert) DemonstrateCoalescing(size int) {
    fmt.Println("üéØ Advanced Memory Coalescing")
    
    // Test different access patterns
    patterns := []struct {
        name        string
        strideFunc  func(int) int
        description string
    }{
        {"Coalesced", func(i int) int { return i }, "Sequential access - optimal"},
        {"Strided-2", func(i int) int { return i * 2 }, "Stride of 2 - suboptimal"},
        {"Strided-4", func(i int) int { return i * 4 }, "Stride of 4 - poor"},
        {"Random", func(i int) int { return (i * 1103515245 + 12345) % size }, "Random - worst"},
    }
    
    data, _ := memory.Alloc(int64(size * 4))
    result, _ := memory.Alloc(int64(size * 4))
    defer data.Free()
    defer result.Free()
    
    // Initialize source data
    srcData := make([]float32, size)
    for i := range srcData {
        srcData[i] = float32(i)
    }
    data.CopyFromHost(srcData)
    
    fmt.Println("\nMemory Access Pattern Analysis:")
    fmt.Println("Pattern\t\tTime\t\tBandwidth\tEfficiency")
    
    for _, pattern := range patterns {
        bandwidth, efficiency := m.benchmarkAccessPattern(data, result, size, pattern.strideFunc)
        fmt.Printf("%-12s\t%.2fms\t\t%.1f GB/s\t%.1f%%\n", 
                   pattern.name, float64(time.Millisecond.Nanoseconds())/1e6, 
                   bandwidth, efficiency)
    }
}

func (m *MemoryExpert) benchmarkAccessPattern(src, dst *memory.DeviceMemory, size int, strideFunc func(int) int) (float64, float64) {
    // Simulate different access patterns
    srcData := make([]float32, size)
    src.CopyToHost(srcData)
    
    dstData := make([]float32, size)
    
    start := time.Now()
    
    // Simulate access pattern
    for i := 0; i < size; i++ {
        srcIdx := strideFunc(i)
        if srcIdx < size {
            dstData[i] = srcData[srcIdx] * 2.0 // Simple computation
        }
    }
    
    elapsed := time.Since(start)
    
    dst.CopyFromHost(dstData)
    
    // Calculate bandwidth
    bytesTransferred := float64(size * 8) // Read + write
    bandwidth := (bytesTransferred / elapsed.Seconds()) / (1024 * 1024 * 1024)
    
    // Theoretical peak bandwidth (example: 900 GB/s for high-end GPU)
    theoreticalPeak := 900.0
    efficiency := (bandwidth / theoreticalPeak) * 100
    
    return bandwidth, efficiency
}

// Shared memory bank conflict analysis
func (m *MemoryExpert) AnalyzeBankConflicts() {
    fmt.Println("\nüè¶ Shared Memory Bank Conflict Analysis")
    
    // Simulate different shared memory access patterns
    bankSize := 32    // 32 banks in shared memory
    elementsPerBank := 32
    
    patterns := []struct {
        name      string
        conflicts int
        desc      string
    }{
        {"No Conflict", 0, "Each thread accesses different bank"},
        {"2-way Conflict", 2, "2 threads access same bank"},
        {"4-way Conflict", 4, "4 threads access same bank"},
        {"Broadcast", 0, "All threads read same address"},
    }
    
    fmt.Println("\nShared Memory Access Patterns:")
    fmt.Println("Pattern\t\tConflicts\tPerformance Impact")
    
    for _, pattern := range patterns {
        impact := m.calculateBankConflictImpact(pattern.conflicts)
        fmt.Printf("%-15s\t%d-way\t\t%.1fx slower\n", 
                   pattern.name, pattern.conflicts, impact)
    }
    
    // Recommendations
    fmt.Println("\nüí° Bank Conflict Optimization:")
    fmt.Println("  - Pad shared memory arrays to avoid conflicts")
    fmt.Println("  - Use __ldg() for read-only data")
    fmt.Println("  - Consider data layout transformations")
    fmt.Println("  - Profile with nvprof/nsight for actual conflicts")
}

func (m *MemoryExpert) calculateBankConflictImpact(conflicts int) float64 {
    if conflicts <= 1 {
        return 1.0 // No slowdown
    }
    return float64(conflicts) // N-way conflict = N times slower
}

// Demonstrate memory prefetching
func (m *MemoryExpert) DemonstratePrefetching(size int) {
    fmt.Println("\nüì° Memory Prefetching Optimization")
    
    data, _ := memory.Alloc(int64(size * 4))
    result, _ := memory.Alloc(int64(size * 4))
    defer data.Free()
    defer result.Free()
    
    // Initialize test data
    testData := make([]float32, size)
    for i := range testData {
        testData[i] = float32(i % 1000)
    }
    data.CopyFromHost(testData)
    
    // Test without prefetching
    start := time.Now()
    m.computeWithoutPrefetch(data, result, size)
    timeWithout := time.Since(start)
    
    // Test with prefetching
    start = time.Now()
    m.computeWithPrefetch(data, result, size)
    timeWith := time.Since(start)
    
    improvement := float64(timeWithout-timeWith) / float64(timeWithout) * 100
    
    fmt.Printf("Without prefetch: %v\n", timeWithout)
    fmt.Printf("With prefetch: %v\n", timeWith)
    fmt.Printf("Improvement: %.1f%%\n", improvement)
    
    if improvement > 5 {
        fmt.Println("‚úÖ Prefetching provides significant benefit")
    } else {
        fmt.Println("‚ö†Ô∏è Prefetching benefit limited (cache already optimal)")
    }
}

func (m *MemoryExpert) computeWithoutPrefetch(src, dst *memory.DeviceMemory, size int) {
    // Simulate computation without prefetching
    srcData := make([]float32, size)
    src.CopyToHost(srcData)
    
    dstData := make([]float32, size)
    
    // Simple computation without prefetch hints
    for i := 0; i < size; i++ {
        dstData[i] = srcData[i] * 2.0 + 1.0
    }
    
    dst.CopyFromHost(dstData)
}

func (m *MemoryExpert) computeWithPrefetch(src, dst *memory.DeviceMemory, size int) {
    // Simulate computation with prefetching
    srcData := make([]float32, size)
    src.CopyToHost(srcData)
    
    dstData := make([]float32, size)
    
    // Simulate prefetching (in real CUDA, use __builtin_assume_aligned, etc.)
    prefetchDistance := 64 // Cache line size
    
    for i := 0; i < size; i++ {
        // Simulate prefetch of future data
        if i+prefetchDistance < size {
            // __builtin_prefetch(&srcData[i + prefetchDistance], 0, 3);
            // In simulation, we just access it to bring to cache
            _ = srcData[i+prefetchDistance]
        }
        
        dstData[i] = srcData[i] * 2.0 + 1.0
    }
    
    dst.CopyFromHost(dstData)
}

func (m *MemoryExpert) Destroy() {
    m.stream.Destroy()
}

// Demonstration
func main() {
    cuda.Initialize()
    fmt.Println("üß† Expert Memory Optimization")
    
    expert := NewMemoryExpert()
    defer expert.Destroy()
    
    // Test different aspects
    expert.DemonstrateCoalescing(1000000)
    expert.AnalyzeBankConflicts()
    expert.DemonstratePrefetching(500000)
    
    // Comprehensive memory analysis
    performMemoryAnalysis()
}

func performMemoryAnalysis() {
    fmt.Println("\nüìä Comprehensive Memory Analysis")
    
    device := cuda.GetDevice(0)
    props := device.GetProperties()
    
    fmt.Printf("Device: %s\n", props.Name)
    fmt.Printf("Global Memory: %.2f GB\n", float64(props.GlobalMemory)/(1024*1024*1024))
    fmt.Printf("Memory Clock: %d MHz\n", props.MemoryClockRate/1000)
    fmt.Printf("Memory Bus Width: %d bits\n", props.MemoryBusWidth)
    
    // Calculate theoretical bandwidth
    theoreticalBW := float64(props.MemoryClockRate) * float64(props.MemoryBusWidth) * 2 / 8 / 1000 // GB/s
    fmt.Printf("Theoretical Bandwidth: %.1f GB/s\n", theoreticalBW)
    
    fmt.Printf("L2 Cache Size: %d KB\n", props.L2CacheSize/1024)
    fmt.Printf("Shared Memory per Block: %d KB\n", props.SharedMemoryPerBlock/1024)
    fmt.Printf("Registers per Block: %d\n", props.RegistersPerBlock)
    
    fmt.Println("\nüí° Memory Optimization Guidelines:")
    fmt.Println("  1. Maximize coalesced access patterns")
    fmt.Println("  2. Minimize bank conflicts in shared memory") 
    fmt.Println("  3. Use appropriate data types (avoid 64-bit when possible)")
    fmt.Println("  4. Leverage constant memory for read-only data")
    fmt.Println("  5. Consider texture memory for spatial locality")
    fmt.Println("  6. Profile actual memory throughput vs theoretical")
    
    fmt.Println("\nüéØ Advanced Techniques:")
    fmt.Println("  - Vector loads/stores (load4, store4)")
    fmt.Println("  - Memory access pattern analysis")
    fmt.Println("  - Cache-conscious algorithm design")
    fmt.Println("  - Prefetching and memory hints")
}
```

---

## üéØ Module Assessment

### **Expert Knowledge Validation**

1. **Kernel Optimization**: Achieve >80% of theoretical GEMM performance
2. **Warp Programming**: Implement efficient warp-level primitives
3. **Memory Mastery**: Identify and eliminate memory bottlenecks
4. **Architecture Understanding**: Optimize for specific GPU generations

### **Practical Challenge**

Implement a complete custom kernel for a specific domain:
- **Scientific**: N-body simulation with spatial partitioning
- **ML/AI**: Custom activation function with backward pass
- **Financial**: Monte Carlo options pricing with variance reduction
- **Graphics**: Real-time ray-triangle intersection

### **Success Criteria**

- ‚úÖ Custom kernel outperforms library implementations by >20%
- ‚úÖ Memory bandwidth utilization >85% of theoretical
- ‚úÖ Warp occupancy >75% across different input sizes
- ‚úÖ Zero bank conflicts in shared memory usage

---

## üöÄ Next Steps

**Congratulations! You've mastered kernel-level optimization.**

**You're now ready for:**
‚û°Ô∏è **[Module 2: Multi-GPU & Distributed Computing](TRAINING_EXPERT_2_DISTRIBUTED.md)**

**Skills Mastered:**
- üî• Custom high-performance kernel development  
- ‚ö° Assembly-level optimization techniques
- üß† GPU microarchitecture exploitation
- üìä Memory hierarchy optimization

---

*From kernel novice to GPU architecture expert - the foundation of elite performance! üèóÔ∏è‚ö°*
